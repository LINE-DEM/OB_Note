# 第二大块：LLM 底层机制

> 在上一大块中，我们了解了 AI Agent 的整体框架——它怎么循环运转、怎么使用工具、怎么记忆和规划。但我们一直在把 LLM（大语言模型）当作一个"黑箱"来看待：输入文字，输出文字，中间发生了什么？
>
> 这一大块，我们要打开这个黑箱，看看 LLM 内部到底在做什么。理解这些底层机制，你才能真正明白：为什么 Prompt 写法不同结果会差很多？为什么模型有时会"胡说八道"？为什么长文章容易出问题？
>
> 不用担心，我们不会涉及数学公式。我会用最直观的方式，让你建立正确的直觉。

---

## 2.1 Token 机制



### Token 切分

现代 LLM 使用的是一种叫 **BPE（Byte Pair Encoding）** 或类似算法的分词方式。

基本原则是：**常见的字符组合会被合并成一个 Token，罕见的组合会被拆开**。

**英文的切分示例：**
```
"Hello world" → ["Hello", " world"]  (2 个 tokens)
"unhappiness" → ["un", "happiness"]   (2 个 tokens)
"ChatGPT" → ["Chat", "G", "PT"]       (3 个 tokens)
```

**中文的切分示例：**
```
"你好" → ["你", "好"]           (2 个 tokens)
"人工智能" → ["人工", "智能"]    (2 个 tokens)
"今天天气不错" → ["今天", "天气", "不", "错"]  (4 个 tokens)
```

注意：不同的模型使用不同的分词器，所以同样的文字在不同模型里可能被切成不同数量的 Token。


### Token需要注意


1. **计数问题**：让 LLM 数"strawberry 里有几个 r"，它可能数错，因为它看到的是 tokens，不是字母。

2. **拼写敏感**：`color` 和 `colour` 是不同的 tokens，模型对它们的理解可能略有不同。

3. **语言效率差异**：同样的内容，中文可能比英文消耗更多 tokens（因为中文字符在训练数据中没那么常见）。




### 实际影响示例

假设你的 Context Window 是 100K tokens：

```
系统指令：2000 tokens
对话历史：50000 tokens
当前问题 + 附带的代码文件：30000 tokens
────────────────────────
已用：82000 tokens
剩余：18000 tokens（模型回复 + 后续对话的空间）
```

如果你附带的代码文件太大，就会挤占回复空间，导致模型的回复被截断或对话很快"撞墙"。

---

## 2.2 Transformer 架构与 Attention 机制

### Transformer 是什么

Transformer 是一种神经网络架构，是现代所有大语言模型（GPT、Claude、Llama 等）的基础结构。2017 年由 Google 提出，论文标题是《Attention Is All You Need》。

在 Transformer 之前，处理语言的主流架构是 RNN（循环神经网络）。RNN 按顺序处理文字，一个词一个词地读，速度慢，而且容易"忘记"前面读过的内容。

Transformer 的革命性在于：**它可以同时看到所有的文字，并且能灵活地关注任何位置的内容**。


### Attention 机制是什么

Attention（注意力）是 Transformer 的核心机制。它解决的问题是：**在处理当前内容时，应该关注前文的哪些部分？**

想象模型正在理解这句话：
> "小明把苹果给了小红，因为**她**很饿。"

当模型读到"她"的时候，它需要知道"她"指的是谁。Attention 机制的作用就是：让"她"这个位置的处理，能够"关注"到前面的"小红"，从而理解"她"="小红"。

```
[小明] [把] [苹果] [给了] [小红] [因为] [她] [很饿]
                              ↑
                           Attention
                         这个"她"在关注谁？
                              │
                    ┌─────────┴─────────┐
                   低关注              高关注
                  [小明]              [小红]
```

Attention 会计算当前位置和所有前文位置的"相关性分数"，然后根据分数来决定该关注哪些内容。


### Self-Attention：自己和自己对话

Transformer 使用的是 **Self-Attention（自注意力）**。"Self"的意思是：序列中的每个位置都会和**同一个序列中**的所有其他位置计算关注度。

简单说：每个词都在问"我应该关注句子里的哪些其他词？"

```
输入："我 喜欢 吃 苹果"

处理"苹果"时，Self-Attention 计算它和每个词的相关性：
- 和"我"的相关性：0.1（低）
- 和"喜欢"的相关性：0.2（中等）
- 和"吃"的相关性：0.5（高——因为"吃苹果"是常见搭配）
- 和自己的相关性：0.2（作为对比）
```

这个机制让模型能够捕捉到词与词之间的关系，无论它们在句子中相距多远。

### Multi-Head Attention：多角度观察

实际的 Transformer 使用 **Multi-Head Attention（多头注意力）**。简单说就是：同时从多个角度计算注意力。

为什么需要"多头"？因为词与词之间的关系是多维度的：

- 语法角度："苹果"和"吃"有动宾关系
- 语义角度："苹果"和"水果"有上下位关系
- 位置角度："苹果"和相邻的词关系更紧密

每个"头"可以学习关注不同维度的关系。多个头组合起来，模型就能从多角度理解文本。


### 为什么 Transformer 适合处理语言

Transformer 相比之前的架构，有三个关键优势：

**1. 并行处理能力**
RNN 必须一个词一个词地顺序处理。Transformer 可以同时处理所有词，训练速度大幅提升。这让训练超大模型成为可能。

**2. 长距离依赖**
"她"和"小红"可能相隔很远，RNN 容易忘记。Transformer 通过 Attention 直接建立连接，不管距离多远。

**3. 灵活的注意力模式**
模型可以学习各种复杂的关注模式：有时候关注相邻词，有时候关注句首，有时候关注特定的关键词。这种灵活性让它能处理各种语言现象。


---

## 2.3 LLM 的推理和生成过程

### 生成回复的实际流程

当你向 LLM 提问时，它生成回复的过程是这样的：

**第一步：分词（Tokenization）**
把你的输入文字切分成 Token 序列。

```
输入："什么是机器学习？"
Token 序列：["什么", "是", "机器", "学习", "？"]
```

**第二步：编码（Encoding）**
把每个 Token 转换成一个数字向量（高维空间中的一个点）。这个向量包含了 Token 的"语义信息"。

**第三步：模型计算（Forward Pass）**
Token 向量通过 Transformer 的多层处理：
- 每一层都包含 Self-Attention 和前馈网络
- 每一层都在提取更高级的特征
- 最终得到每个位置的"输出向量"

**第四步：预测下一个 Token**
根据最后一个位置的输出向量，计算词汇表中每个 Token 作为"下一个词"的概率。

```
下一个 Token 的概率分布：
"机器" → 5%
"学习" → 8%
"是" → 3%
"一种" → 15%   ← 概率最高
"指" → 12%
...
```

**第五步：采样（Sampling）**
根据概率分布选择一个 Token 作为输出。最简单的方法是选概率最高的（贪婪采样），但通常会用更复杂的采样策略。

**第六步：循环生成**
把新生成的 Token 加到输入序列末尾，重复第二到第五步，直到生成结束符或达到长度限制。

```
输入：["什么", "是", "机器", "学习", "？"]
第1轮生成：["什么", "是", "机器", "学习", "？", "机器"]
第2轮生成：["什么", "是", "机器", "学习", "？", "机器", "学习"]
第3轮生成：["什么", "是", "机器", "学习", "？", "机器", "学习", "是"]
...
```

> **类比：接龙游戏**
>
> LLM 生成文本的方式本质上是"超级复杂的文字接龙"：
> 1. 看到目前为止的所有文字
> 2. 预测下一个最合适的词是什么
> 3. 把这个词加上去
> 4. 重复这个过程
>
> 它不是一次性"想好整个回答"，而是一个词一个词地"接"出来。这就是为什么它叫"语言**模型**"——它在模拟"给定前文，下一个词是什么"这个概率分布。

### 自回归生成（Autoregressive Generation）

这种"逐词生成"的方式有一个专业名称：**自回归生成（Autoregressive Generation）**。

"自回归"的意思是：每一步的输出都依赖于之前所有的输出。

```
P(回复) = P(词1) × P(词2|词1) × P(词3|词1,词2) × P(词4|词1,词2,词3) × ...
```

用人话说：生成第 N 个词的时候，模型会参考前 N-1 个词。



**2. 为什么开头很重要**
如果开头就跑偏了，后面会一直基于错误的开头继续生成，越跑越偏。

**3. 为什么长回复更容易出错**
每多生成一个词，就多一次出错的机会。错误会累积。

> **类比：多米诺骨牌**
>
> 自回归生成就像推多米诺骨牌。第一张牌的方向决定了后面所有牌的走向。如果中间有一张歪了，后面都会跟着歪。
>
> 这就是为什么 Prompt 的开头部分特别重要——它会影响整个回复的方向。

### 为什么 LLM 会产生"幻觉"

"幻觉"（Hallucination）是指 LLM 一本正经地说出**错误的、虚构的信息**，比如编造不存在的论文引用、虚构历史事件、说错代码函数的参数。

从机制上理解，幻觉的产生有几个原因：

**1. 模型只是在做"概率预测"，不是在"查阅知识库"**

模型不是数据库。它不会去"查"某个事实是否正确。它只是在预测"给定这个上下文，下一个最可能的词是什么"。

如果"听起来对"的表述在训练数据中很常见，模型就会生成它，哪怕内容是错的。

```
问："爱因斯坦是哪年发表狭义相对论的？"
模型推理过程（简化）：
- "爱因斯坦...发表...相对论...年份..."
- 训练数据中见过很多"1905年"和"狭义相对论"一起出现
- 输出"1905年"（碰巧是对的）

问："XX大学的YY教授发表了什么论文？"
模型推理过程：
- "教授...论文...标题..."
- 训练数据中没见过这个具体信息
- 但见过很多"XX领域的论文标题模式"
- 输出一个"听起来很像"的论文标题（但是编的）
```

**2. 模型倾向于给出自信的回答**

LLM 被训练成"有帮助的助手"。当它不确定时，它通常不会说"我不知道"，而是会生成一个"看起来合理"的回答。

**3. 概率采样的随机性**

生成过程中有随机性（由 Temperature 等参数控制）。有时候会采样到概率不是最高的选项，导致回复走向奇怪的方向。

> **类比：即兴演讲**
>
> 想象一个人即兴演讲。他不是在"读稿"，而是根据"什么听起来合适"在临场组织语言。
>
> 如果他对某个话题不熟悉，他可能会"编"一些听起来合理的内容来填充。他不是故意撒谎，只是在即兴发挥时，"听起来对"和"实际上对"是两回事。
>
> LLM 的幻觉本质上就是这种"即兴发挥的失误"。

### Temperature 等参数是怎么影响生成的

生成过程中有几个关键参数：

**Temperature（温度）**

控制生成的"随机性"或"创造性"。

- **Temperature = 0**：总是选概率最高的 Token（确定性输出）
- **Temperature = 1**：按原始概率分布采样（标准随机）
- **Temperature > 1**：概率分布被"平滑"，不太可能的选项也有更大机会被选中（更随机）
- **Temperature < 1**：概率分布被"锐化"，高概率选项更容易被选中（更确定）

```
例子：下一个词的概率分布
原始：{"好的": 0.6, "可以": 0.25, "没问题": 0.1, "当然": 0.05}

Temperature = 0.5 后：{"好的": 0.8, "可以": 0.15, "没问题": 0.04, "当然": 0.01}
→ "好的"几乎肯定被选中

Temperature = 1.5 后：{"好的": 0.4, "可以": 0.3, "没问题": 0.2, "当然": 0.1}
→ 各选项都有不小的概率被选中
```

> **类比：点菜的决策**
>
> - Temperature = 0：每次都点"最喜欢的那道菜"
> - Temperature = 0.5：大概率点喜欢的，偶尔尝试一下第二喜欢的
> - Temperature = 1：按"喜欢程度"的比例随机选
> - Temperature = 1.5：什么都可能点，包括平时不太会选的菜

**Top-P（Nucleus Sampling）**

只考虑概率累计达到 P 的那些选项。

Top-P = 0.9 意味着：只从概率最高的、累计概率达到 90% 的词里面选。

**Top-K**

只考虑概率最高的 K 个选项。

**实际应用建议：**

| 场景    | 推荐设置                     | 原因         |
| ----- | ------------------------ | ---------- |
| 写代码   | Temperature 低 (0-0.3)    | 需要准确、一致的输出 |
| 写创意文案 | Temperature 中高 (0.7-1.0) | 需要多样性和创造力  |
| 问答/分析 | Temperature 中低 (0.3-0.7) | 需要准确但不要太死板 |
| 头脑风暴  | Temperature 高 (1.0+)     | 需要打破常规思维   |

---

## 2.4 Prompt 对 LLM 的底层影响

### Prompt 在机制层面是怎么影响输出的

理解了前面的内容，我们现在可以深入分析：**为什么 Prompt 这么重要？它到底在底层产生了什么影响？**

**1. Prompt 决定了初始的 Context**

LLM 生成的每一个词都依赖于之前的所有内容。Prompt 是生成过程的"起点"，它决定了模型在什么"语境"下开始思考。

```
Prompt A："写一首诗"
→ 模型进入"诗歌创作"模式，生成风格偏文学

Prompt B："用代码实现..."
→ 模型进入"编程"模式，生成风格偏技术

Prompt C："假设你是一个资深程序员，请用代码实现..."
→ 模型进入"专家程序员"模式，可能生成更专业的代码
```

**2. Prompt 的措辞影响概率分布**

不同的措辞会激活模型中不同的"知识区域"，影响下一个词的概率分布。

```
"告诉我Python的列表是什么"
→ 激活训练数据中"Python教程"相关的模式
→ 输出偏向教程风格

"用一句话解释Python的列表"
→ 激活"简洁定义"相关的模式
→ 输出偏向精简

"像对五岁小孩一样解释Python的列表"
→ 激活"科普/通俗解释"相关的模式
→ 输出偏向简单易懂
```

**3. Prompt 的结构影响模型的"思维路径"**

如果你在 Prompt 中展示了某种结构，模型会倾向于生成类似结构的回复。

```
Prompt："
问题：2+2=？
思考过程：2加2等于4
答案：4

问题：5+3=？"

模型会倾向于生成：
"思考过程：5加3等于8
答案：8"
```

这就是为什么 Few-shot（少样本示例）有效——你给模型展示了"应该怎么回答"，模型就会模仿。

> **类比：即兴表演的暗示**
>
> 想象一个即兴表演演员。如果你说"假设你是一个海盗"，他会立刻切换到海盗的说话方式和行为模式。
>
> Prompt 对 LLM 的作用也类似——它给模型一个"应该扮演什么角色、用什么方式说话"的暗示，模型就会尽力去匹配这个暗示。

### 为什么好 Prompt 和差 Prompt 差异这么大

现在我们可以从机制角度解释这个现象：

**差 Prompt 的问题：**

```
"帮我写代码"
```

- 没有指定写什么代码
- 没有指定语言
- 没有指定风格
- 模型面对巨大的可能性空间，可能生成不符合预期的东西

**好 Prompt 的优势：**

```
"请用 Python 写一个函数：
- 函数名：calculate_average
- 功能：计算一个数字列表的平均值
- 输入：一个数字列表
- 输出：平均值（float 类型）
- 要求：处理空列表的情况，返回 0"
```

- 明确指定了语言（Python）
- 明确指定了函数名
- 明确指定了输入输出
- 明确指定了边界条件
- 模型的"预测空间"被大大缩小，更容易生成你想要的东西

**机制层面的解释：**

好的 Prompt 通过提供更多约束，**缩小了概率分布的"有效范围"**。

想象概率分布是一个山峰。差的 Prompt 让这个山峰非常扁平，各种输出都有可能。好的 Prompt 让山峰变尖，正确的输出概率大大提升。

```
差 Prompt 的概率分布：
[JavaScript实现]: 15%
[Python实现]: 15%
[伪代码]: 10%
[概念解释]: 10%
[其他]: 50%

好 Prompt 的概率分布：
[Python calculate_average 函数]: 70%
[稍有变化的正确实现]: 20%
[其他]: 10%
```

> **类比：问路**
>
> 差问法："附近有吃的吗？" → 可能推荐火锅、快餐、咖啡厅... 各种可能
>
> 好问法："步行10分钟以内，有没有人均50元以下的川菜馆？" → 范围大大缩小，答案更精准
>
> Prompt 的精确程度决定了回答的精准程度。

### 几个关键的 Prompt 设计原则（基于底层机制）

理解了机制，我们可以推导出一些 Prompt 设计原则：

**1. 角色设定激活专业知识区**

```
"你是一个有10年经验的后端工程师..."
```

这个设定会激活训练数据中"后端工程师"相关的模式，让模型更倾向于生成专业的回答。

**2. 示例引导格式和风格**

```
"输入：苹果
输出：水果

输入：狗
输出：动物

输入：电脑
输出："
```

通过示例，你告诉模型"我期望的回复格式是什么"。

**3. 分步骤指令降低错误率**

```
"请按以下步骤处理：
1. 首先分析问题
2. 然后列出可能的解决方案
3. 最后选择最优方案并解释"
```

这种分步指令让模型走"更安全"的推理路径，降低一步到位出错的风险。

**4. 明确约束缩小输出空间**

```
"用不超过100字回答..."
"只回答是或否..."
"用JSON格式输出..."
```

约束越明确，模型"猜错"你意图的可能性越小。

**5. 提供上下文激活相关记忆**

```
"这是一个 React 项目，使用 TypeScript，遵循函数式组件风格..."
```

这些上下文信息会激活模型中"React + TypeScript + 函数式组件"的知识区域。

---

## 本章小结

让我们回顾 LLM 底层机制的核心要点：

| 概念 | 核心要点 | 一句话记忆 |
|------|---------|-----------|
| Token | 文本处理的最小单位 | LLM 的"乐高积木" |
| Transformer | 支持并行处理和长距离关联的架构 | "一目十行"的速读高手 |
| Attention | 让模型知道该关注哪些前文内容 | "把注意力放在重要的地方" |
| 自回归生成 | 逐 Token 预测和生成 | 超级复杂的"文字接龙" |
| 幻觉 | 模型输出"听起来对但实际错"的内容 | "即兴发挥的失误" |
| Temperature | 控制生成的随机性/创造性 | "点菜的冒险程度" |
| Prompt 影响 | 决定初始 Context，缩小概率分布范围 | "给演员的表演指令" |

**核心认知转变：**

1. LLM **不是知识库**，而是"概率预测器"——它在预测"下一个最可能的词"，不是在"查询事实"
2. 生成是**逐步进行**的——一个词一个词地"接龙"，不是一次性"想好答案"
3. Prompt 的作用是**缩小可能性空间**——越明确的 Prompt，越容易得到想要的结果
4. 幻觉是**机制的副产品**，不是 bug——理解这一点才能正确使用 LLM


